{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amritanjali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amritanjali\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "# Cleaning the texts\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Liked\n",
      "0                             Wow... Loved this place.      1\n",
      "1                                   Crust is not good.      0\n",
      "2            Not tasty and the texture was just nasty.      0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "5       Now I am getting angry and I want my damn pho.      0\n",
      "6                Honeslty it didn't taste THAT fresh.)      0\n",
      "7    The potatoes were like rubber and you could te...      0\n",
      "8                            The fries were great too.      1\n",
      "9                                       A great touch.      1\n",
      "10                            Service was very prompt.      1\n",
      "11                                  Would not go back.      0\n",
      "12   The cashier had no care what so ever on what I...      0\n",
      "13   I tried the Cape Cod ravoli, chicken, with cra...      1\n",
      "14   I was disgusted because I was pretty sure that...      0\n",
      "15   I was shocked because no signs indicate cash o...      0\n",
      "16                                 Highly recommended.      1\n",
      "17              Waitress was a little slow in service.      0\n",
      "18   This place is not worth your time, let alone V...      0\n",
      "19                                did not like at all.      0\n",
      "20                                 The Burrittos Blah!      0\n",
      "21                                  The food, amazing.      1\n",
      "22                               Service is also cute.      1\n",
      "23   I could care less... The interior is just beau...      1\n",
      "24                                  So they performed.      1\n",
      "25   That's right....the red velvet cake.....ohhh t...      1\n",
      "26          - They never brought a salad we asked for.      0\n",
      "27   This hole in the wall has great Mexican street...      1\n",
      "28   Took an hour to get our food only 4 tables in ...      0\n",
      "29                   The worst was the salmon sashimi.      0\n",
      "..                                                 ...    ...\n",
      "970  I immediately said I wanted to talk to the man...      0\n",
      "971                    The ambiance isn't much better.      0\n",
      "972  Unfortunately, it only set us up for disapppoi...      0\n",
      "973                              The food wasn't good.      0\n",
      "974  Your servers suck, wait, correction, our serve...      0\n",
      "975      What happened next was pretty....off putting.      0\n",
      "976  too bad cause I know it's family owned, I real...      0\n",
      "977               Overpriced for what you are getting.      0\n",
      "978               I vomited in the bathroom mid lunch.      0\n",
      "979  I kept looking at the time and it had soon bec...      0\n",
      "980  I have been to very few places to eat that und...      0\n",
      "981  We started with the tuna sashimi which was bro...      0\n",
      "982                            Food was below average.      0\n",
      "983  It sure does beat the nachos at the movies but...      0\n",
      "984       All in all, Ha Long Bay was a bit of a flop.      0\n",
      "985  The problem I have is that they charge $11.99 ...      0\n",
      "986  Shrimp- When I unwrapped it (I live only 1/2 a...      0\n",
      "987     It lacked flavor, seemed undercooked, and dry.      0\n",
      "988  It really is impressive that the place hasn't ...      0\n",
      "989  I would avoid this place if you are staying in...      0\n",
      "990  The refried beans that came with my meal were ...      0\n",
      "991         Spend your money and time some place else.      0\n",
      "992  A lady at the table next to us found a live gr...      0\n",
      "993            the presentation of the food was awful.      0\n",
      "994           I can't tell you how disappointed I was.      0\n",
      "995  I think food should have flavor and texture an...      0\n",
      "996                           Appetite instantly gone.      0\n",
      "997  Overall I was not impressed and would not go b...      0\n",
      "998  The whole experience was underwhelming, and I ...      0\n",
      "999  Then, as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
